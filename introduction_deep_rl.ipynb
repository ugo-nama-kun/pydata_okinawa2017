{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras-rl で深層強化学習入門\n",
    "\n",
    "## 深層強化学習とは？\n",
    "\n",
    "\n",
    "\n",
    "## keras-rl とは？\n",
    " [Karlsruhe Institute of Technology](http://www.kit.edu/) (カールスルーエ工科大学， KIT) の人たちがつくった keras っぽく深層強化学習が使える python package． \n",
    " \n",
    " Kerasをベースにしている．\n",
    " \n",
    "[keras-rl @ github](https://github.com/matthiasplappert/keras-rl)\n",
    "\n",
    "### 特徴\n",
    "* Pros\n",
    "    * 使いやすい！ （Kerasのようなインターフェイス， OpenAI Gymに準拠）\n",
    "    * 様々な深層強化学習のアルゴリズムが利用できる\n",
    "    * Keras でモデルを作ってそのまま渡せる\n",
    "* Cons\n",
    "    * まだドキュメント化が不十分\n",
    "    \n",
    "### サポートするアルゴリズム\n",
    "#### 離散行動（出力が離散的値．right, left, up,..）\n",
    "* Deep Q Learning (DQN)\n",
    "* Double DQN\n",
    "* Cross-Entropy Method (CEM)\n",
    "* Dueling network DQN (Dueling DQN)\n",
    "\n",
    "#### 連続行動（出力が連続ベクトル）\n",
    "* Deep Deterministic Policy Gradient (DDPG)\n",
    "* Continuous DQN (CDQN or NAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数値計算に必須のもろもろ\n",
    "import numpy as np\n",
    "\n",
    "# 可視化パッケージ \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "# seaborn を入れてない人は以下をコメントアウト\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym で環境の導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OpenAI Gym\n",
    "import gym\n",
    "\n",
    "# 離散行動の環境を指定 （後でDQNをつかうため）\n",
    "ENV_NAME ='CartPole-v0'\n",
    "#ENV_NAME ='Acrobot-v0'\n",
    "\n",
    "# 環境の初期化\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "\n",
    "# インターフェイスの情報をもらっておく\n",
    "nb_actions = env.action_space.n\n",
    "input_shape = (1,) + env.observation_space.shape\n",
    "\n",
    "print(\"# of Actions : {}\".format(nb_actions))\n",
    "print(\"Shape of Observation : {}\".format(env.observation_space.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras の Sequential Model でネットワークを作成\n",
    "訓練に使うネットワークをKerasのSequential model として作成します．\n",
    "\n",
    "入力がObservationの次元数，出力が離散行動の数（nb_actions）のネットワークを作成します．\n",
    "<img src=\"images/network.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "\n",
    "# 深層（笑）パーセプトロン\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=input_shape))\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Network エージェントの作成\n",
    "Deep Q Network （DQN） エージェントを作成します．\n",
    "\n",
    "DQNエージェントは Replay Buffer と呼ばれる，これまでの環境との相互作用データを残しておきます（SequentialMemory）．\n",
    "\n",
    "Episode の結果だけを使う場合，結果だけを残すEpisodeParameterMemoryもあります．この形のメモリはCross Entropy Method 内で使われています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deep Reinforcement Learning: keras-rl\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "\n",
    "# Optimizer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# メモリーバッファーをつくる\n",
    "memory = SequentialMemory(limit=1000, window_length=1)\n",
    "\n",
    "# 方策をつくる\n",
    "#policy = BoltzmannQPolicy(tau=1.0)\n",
    "policy =EpsGreedyQPolicy(eps=0.1)\n",
    "\n",
    "# Reward の大きさをスケーリング\n",
    "class RewardProcessor(Processor):\n",
    "    def process_reward(self, reward):\n",
    "        return reward / 1.\n",
    "processor = RewardProcessor()\n",
    "\n",
    "# Deep Q Network をつくる\n",
    "dqn = DQNAgent(model=model,\n",
    "               gamma = 0.95,\n",
    "               nb_actions=nb_actions,\n",
    "               memory=memory,\n",
    "               nb_steps_warmup=10,\n",
    "               target_model_update=1e-2,\n",
    "               policy=policy,\n",
    "               processor=processor)\n",
    "\n",
    "# Optimizer を設定\n",
    "optimizer = Adam(lr=1e-3, epsilon=0.0001)\n",
    "\n",
    "#  dqn エージェントの作成\n",
    "# MAE: mean_absolute_error でエラー表示\n",
    "dqn.compile(optimizer=optimizer, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可視化用のcallback を定義\n",
    "callback　関数を定義して，plotでの可視化を導入します．\n",
    "\n",
    "callback（rl.callbacks）はkerasの [CallBack](https://keras.io/ja/callbacks/#_2) を拡張したもので，強化学習に関するデータがlogsに追加されています．\n",
    "\n",
    "（ただし，validation dataが無いので使えないcallbackもある）\n",
    "\n",
    "今回はこれでパフォーマンスの可視化をします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rl.callbacks import Callback, ModelIntervalCheckpoint\n",
    "\n",
    "class PlotReward(Callback):\n",
    "    def on_train_begin(self, episode, logs={}):\n",
    "        self.episode_reward = []\n",
    "        self.fig = plt.figure(0)\n",
    "\n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        self.episode_reward.append(logs['episode_reward'])\n",
    "        self.show_result()\n",
    "\n",
    "    def show_result(self):\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        plt.clf()\n",
    "        plt.plot(self.episode_reward, 'r')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.pause(0.001)\n",
    "\n",
    "callbacks = [PlotReward(), ModelIntervalCheckpoint(filepath='./weight_now.h5f', interval=1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実際に学習サイクルを回します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 学習\n",
    "dqn.fit(env, nb_steps=10000, visualize=True, callbacks=callbacks, nb_max_episode_steps=1000)\n",
    "\n",
    "# 学習後， 環境を消す\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Episode分だけネットワークを固定してテストしてみます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True, nb_max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習されたモデルを保存します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# モデルのweight の保存\n",
    "#dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# モデルをそのまま保存\n",
    "dqn.model.save('mymdel.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存したモデルを読み込んで，再び試してみます\n",
    "保存したモデルを load して，同じ環境で動かしてみます．\n",
    "\n",
    "学習済みのモデルを保存しておけば，訓練せずに後から使えます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# 前のモデルを一旦消す\n",
    "del model\n",
    "\n",
    "# 学習したモデルをLoad\n",
    "model = load_model('mymdel.h5f')\n",
    "\n",
    "# observationから行動を取ってくる関数\n",
    "policy = BoltzmannQPolicy()\n",
    "def get_action(observation):\n",
    "    q_values = model.predict(observation)\n",
    "    return policy.select_action(q_values[0])\n",
    "\n",
    "\n",
    "# 同じ環境を新たに作って試す\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "\n",
    "# 1Episode だけ試して終了\n",
    "done = False\n",
    "observation = env.reset()\n",
    "while not done:\n",
    "    action = get_action(observation.reshape(1,1,4))\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# オリジナルの環境で強化学習してみよう\n",
    "強化学習を「つかう」という視点で考えると，\n",
    "\n",
    "* 強化学習パッケージを知っている（keras-rl）\n",
    "* 強化学習のアルゴリズムを知っている（dqn, ddpg,...）\n",
    "* 強化学習の環境を作ることができる <-- いまここ\n",
    "\n",
    "## そういうことで，あとは自分で新しい環境をつくってみよう！\n",
    "参考資料：　[inoory さんの qiita 記事](http://qiita.com/inoory/items/e63ade6f21766c7c2393)\n",
    "\n",
    "[新しい環境を作る時に必要なもの](https://github.com/openai/gym/blob/master/gym/core.py#L27)：\n",
    "\n",
    "以下のmethodを自作の環境ではオーバーライドしなければいけない:\n",
    "* _step\n",
    "* _reset\n",
    "* _render\n",
    "* _close\n",
    "* _configure\n",
    "* _seed\n",
    "\n",
    "また，以下を埋める必要がある\n",
    "* action_space: action の範囲を指定する Space object\n",
    "* observation_space: observation の範囲を指定する Space object\n",
    "* reward_range: 報酬の最大・最小値の値域を指定\n",
    "\n",
    "最低限，以下を実装すればokらしい\n",
    "* _step\n",
    "* _reset\n",
    "* action_space\n",
    "* observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# seaborn を入れてない人は以下をコメントアウト\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleVision(gym.core.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    def __init__(self):\n",
    "        self._n_fruit = 2\n",
    "        self._pos = np.random.randint(size=2, low=0, high=5)\n",
    "        self._fruit_pos = np.random.randint(size=(self._n_fruit, 2), low=0, high=5)\n",
    "        \n",
    "        # 行動は上下左右\n",
    "        self.action_space = gym.spaces.Discrete(4) \n",
    "        \n",
    "        # G G G G G\n",
    "        # G R G G G\n",
    "        # G G G R G\n",
    "        # G G B G G\n",
    "        # G G G G G\n",
    "        # R: fruit, G: background, B: agent\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(1, 5*5*3))\n",
    "    \n",
    "    def _reset(self):\n",
    "        self._pos = np.random.randint(size=2, low=0, high=5)\n",
    "        self._fruit_pos = np.random.randint(size=(self._n_fruit, 2), low=0, high=5)\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _step(self, action):\n",
    "        # 移動\n",
    "        if action == 0: # UP\n",
    "            self._pos[1] += 1\n",
    "            if self._pos[1] >= 5:\n",
    "                self._pos[1] = 4\n",
    "        elif action == 1: # DOWN\n",
    "            self._pos[1] -= 1\n",
    "            if self._pos[1] < 0:\n",
    "                self._pos[1] = 0\n",
    "        elif action == 2: # LEFT\n",
    "            self._pos[0] += 1\n",
    "            if self._pos[0] >= 5:\n",
    "                self._pos[0] = 4\n",
    "        elif action == 3: # RIGHT\n",
    "            self._pos[0] -= 1\n",
    "            if self._pos[0] < 0:\n",
    "                self._pos[0] = 0\n",
    "        \n",
    "        # 報酬設定\n",
    "        reward = -0.1\n",
    "        new_fruit_pos = copy.deepcopy(self._fruit_pos)\n",
    "        for i, f_pos in enumerate(self._fruit_pos):\n",
    "            if np.prod(np.isclose(self._pos, f_pos)) == True:\n",
    "                # フルーツの場所に着いたら，報酬\n",
    "                reward = 1.0\n",
    "                # 新しいフルーツを置く\n",
    "                new_fruit_pos[i] = np.random.randint(size=2, low=0, high=5)\n",
    "                while np.prod(np.isclose(self._pos, new_fruit_pos[i])) == True:\n",
    "                    new_fruit_pos[i] = np.random.randint(size=2, low=0, high=5)\n",
    "        self._fruit_pos = copy.deepcopy(new_fruit_pos)\n",
    "                \n",
    "        # 完了条件は特にない\n",
    "        done = False\n",
    "\n",
    "        # 追加情報はないため，最後は空dict\n",
    "        return self._get_observation(), reward, done, {}\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        observation = np.zeros((5, 5, 3))\n",
    "        observation[:,:,1] = 0.5\n",
    "        # Fruit Position\n",
    "        for f_pos in self._fruit_pos:\n",
    "            observation[f_pos[0], f_pos[1]] = [1, 0, 0]\n",
    "        # Agent Position\n",
    "        observation[self._pos[0], self._pos[1]] = [0, 0, 1]\n",
    "        return observation.reshape(1, 5*5*3)\n",
    "\n",
    "    def _render(self, mode='human', close=False):\n",
    "        plt.figure(1)\n",
    "        plt.clf()\n",
    "        plt.imshow(self._get_observation().reshape(5,5,3), interpolation='nearest')\n",
    "        plt.pause(0.0001)\n",
    "    \n",
    "env = SimpleVision()\n",
    "nb_actions = env.action_space.n\n",
    "input_shape = (1,) + env.observation_space.shape\n",
    "#input_shape = env.observation_space.shape\n",
    "\n",
    "print(\"# of Actions : {}\".format(nb_actions))\n",
    "print(\"Shape of Observation : {}\".format(env.observation_space.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Permute\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "\n",
    "# DQNのネットワーク定義\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=input_shape))\n",
    "#model.add(Convolution2D(32, 3, 3, input_shape=input_shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# experience replay用のmemory\n",
    "memory = SequentialMemory(limit=10000, window_length=1)\n",
    "\n",
    "policy = EpsGreedyQPolicy(eps=0.2) \n",
    "dqn = DQNAgent(model=model,\n",
    "               gamma = 0.99,\n",
    "               nb_actions=nb_actions,\n",
    "               memory=memory,\n",
    "               nb_steps_warmup=10,\n",
    "               target_model_update=1e-2,\n",
    "               policy=policy)\n",
    "\n",
    "# Optimizer を設定\n",
    "optimizer = Adam(lr=1e-3, epsilon=0.0001)\n",
    "\n",
    "dqn.compile(optimizer=optimizer, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rl.callbacks import Callback\n",
    "\n",
    "class PlotReward(Callback):\n",
    "    def on_train_begin(self, episode, logs={}):\n",
    "        self.episode_reward = []\n",
    "        self.fig = plt.figure(0)\n",
    "\n",
    "    def on_episode_end(self, episode, logs={}):\n",
    "        self.episode_reward.append(logs['episode_reward'])\n",
    "        self.show_result()\n",
    "\n",
    "    def show_result(self):\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        plt.clf()\n",
    "        plt.plot(self.episode_reward, 'r')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.pause(0.001)\n",
    "        \n",
    "callbacks = [PlotReward()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dqn.fit(env,\n",
    "        nb_steps=50000,\n",
    "        visualize=False,\n",
    "        callbacks = callbacks,\n",
    "        nb_max_episode_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True, nb_max_episode_steps=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
